{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Guide to OSCP I am writing this guide to cover all OSCP topics as well as other infosec knowledge in details, I will also provide a cheat-sheet in each section so that you can use the commands directly once you understand the topics/tools. But I think to become a good pentester you should know how things work. Pentesting is a very wide field, like if you are interested in webapp pentesting then you should know how application interacts with databases, basics of databses, webapp languages. Similarly if you want to become system/platform(OS) pentester, you should know how to setup the OS, how to do system configuration, and what people can miss when they do configuration. This can guide you to think in the direction of finding vulnerability. If you want to go into network pentesting, you should know TC/IP layer, OSI layer, network configuration and basic knowledge of how internet and firewall works. Always remember, information security is a journey, there are emough amount of things to learn and they keep increasing as the technology changes, so only thing that you need to conquer this journey is patience and curiosity. When I started OSCP, I didn't even know about nmap, so it is not that before attempting OSCP you have to learn something. Only prerequisite is: - Basic understanding of OS, and TCP/IP protocol - Also, it would be great if you are familiar with basic commands in linux and windows. - Able to read and understand code written in C, python and bash First phase of pentesting is the \"Information Gathering\" phase, in this the pentester searches for publicly available information about the client and identifies potential ways to connect to its systems. After this, the tester uses this information to determine the value of each finding and the impact to the client if the finding permitted an attacker to break into a system. Second phase is \"Vulnerability Analysis\", this is the phase where you attempt to discover vulnerability in the system, system can be a webapp, any application running on particular OS or the OS itself. Once you find that there is a vulnerability, then you need to find a way to exploit it. Next comes \"Exploitation Phase\" depends on the type/kind of vulnerability and a pentester need to craft a exploit using his/her skill to exploit the software. Fourth phase is \"Post-Exploitation\" phase, where the result of the exploitation is used to find additional information, sensitive data(like password files, password stored in browser, other private files ), access to other systems resources. Then comes the last phase called \"Reporting\" phase, in this, the pentester summarizes the findings, including steps of exploitation and problem with the software which allows attacker to exploit it. I am covering all these phases including some other resource that will help you in OSCP as well as pentesting journey. Content Setting Up Kali Linux","title":"Home"},{"location":"#guide-to-oscp","text":"I am writing this guide to cover all OSCP topics as well as other infosec knowledge in details, I will also provide a cheat-sheet in each section so that you can use the commands directly once you understand the topics/tools. But I think to become a good pentester you should know how things work. Pentesting is a very wide field, like if you are interested in webapp pentesting then you should know how application interacts with databases, basics of databses, webapp languages. Similarly if you want to become system/platform(OS) pentester, you should know how to setup the OS, how to do system configuration, and what people can miss when they do configuration. This can guide you to think in the direction of finding vulnerability. If you want to go into network pentesting, you should know TC/IP layer, OSI layer, network configuration and basic knowledge of how internet and firewall works. Always remember, information security is a journey, there are emough amount of things to learn and they keep increasing as the technology changes, so only thing that you need to conquer this journey is patience and curiosity. When I started OSCP, I didn't even know about nmap, so it is not that before attempting OSCP you have to learn something. Only prerequisite is: - Basic understanding of OS, and TCP/IP protocol - Also, it would be great if you are familiar with basic commands in linux and windows. - Able to read and understand code written in C, python and bash First phase of pentesting is the \"Information Gathering\" phase, in this the pentester searches for publicly available information about the client and identifies potential ways to connect to its systems. After this, the tester uses this information to determine the value of each finding and the impact to the client if the finding permitted an attacker to break into a system. Second phase is \"Vulnerability Analysis\", this is the phase where you attempt to discover vulnerability in the system, system can be a webapp, any application running on particular OS or the OS itself. Once you find that there is a vulnerability, then you need to find a way to exploit it. Next comes \"Exploitation Phase\" depends on the type/kind of vulnerability and a pentester need to craft a exploit using his/her skill to exploit the software. Fourth phase is \"Post-Exploitation\" phase, where the result of the exploitation is used to find additional information, sensitive data(like password files, password stored in browser, other private files ), access to other systems resources. Then comes the last phase called \"Reporting\" phase, in this, the pentester summarizes the findings, including steps of exploitation and problem with the software which allows attacker to exploit it. I am covering all these phases including some other resource that will help you in OSCP as well as pentesting journey.","title":"Guide to OSCP"},{"location":"#content","text":"Setting Up Kali Linux","title":"Content"},{"location":"BugBountyRecon/","text":"Ideally you\u2019re going to be wanting to choose a program that has a wide scope. You\u2019re also going to be wanting to look for a bounty program that has a wider range of vulnerabilities within scope. Mining information about the domains, email servers and social network connections. Bug Bounty Hunting Tip #1- Always read the Source Code 1. Sumdomain Enumeration Enumerate Subdomains Web Tools: https://pentest-tools.com/ https://virustotal.com/ https://www.shodan.io/ https://crt.sh/?q=%25taregt.com https://dnsdumpster.com/ https://censys.io http://dnsgoodies.com Tools: https://bitbucket.org/LaNMaSteR53/recon-ng https://github.com/michenriksen/aquatone https://github.com/aboul3la/Sublist3r https://github.com/rbsec/dnscan https://github.com/Cleveridge/cleveridge-subdomain-scanner Screenshot Tools webscreenshot Make sure to install PhantomJS too. $ git clone https://github.com/maaaaz/webscreenshot.git Once this is done, we use a tool called epg-prep (https://www.npmjs.com/package/epg-prep) to create thumbnails to do so, simply run: epg-prep uber.com This will allow us to view the created pictures using express-photo-gallery. In a final step, use the express-gallery-script from the bottom of this blogpost and save it as yourname.js. All you need to do is to change the folder name inside the script: app.use('/photos', Gallery('uber.com', options)); the folder name in this case is set uber.com but depending on which target you look at it may be different. Once you\u2019ve done that you can simply run the script using node yourname.js. This will create a webserver listening on Port 3000 with an endpoint called /photos. So to access this you simply type: http://yourserverip:3000/photos to get a nice overview of the subdomains you have enumerated System Tools apt update apt upgrade curl -sL https://deb.nodesource.com/setup_6.x | sudo -E bash - apt install -y git wget python python-pip phantomjs xvfb screen slurm gem phantomjs imagemagick graphicsmagick nodejs Requirements for WebScreenshot pip install webscreenshot pip install selenium Requirements for express-photo-gallery sudo npm install -g npm npm install express-photo-gallery npm install express npm install -g epg-prep Requirements for Aquatone git clone https://github.com/michenriksen/aquatone.git cd aquatone/ gem install aquatone express-photo-gallery Script JavaScript var express = require('express'); var app = express(); var Gallery = require('express-photo-gallery'); var options = { title: 'My Awesome Photo Gallery' }; app.use('/photos', Gallery('uber.com', options)); app.listen(3000); Sublist3r $ git clone https://github.com/aboul3la/Sublist3r.git $ cd Sublist3r $ sudo pip install -r requirements.txt alias sublist3r='python /path/to/Sublist3r/sublist3r.py -d ' alias sublist3r-one=\". (cat domains | awk '{print \\\"sublist3r \\\"$1 \\\" -o \\\" $1 \\\".txt\\\"}')\" dirsearch $ git clone https://github.com/maurosoria/dirsearch.git $ cd dirsearch/db $ wget https://gist.githubusercontent.com/EdOverflow/c4d6d8c43b315546892aa5dab67fdd6c/raw/7dc210b17d7742b46de340b824a0caa0f25cf3cc/open_redirect_wordlist.txt alias dirsearch='python3 /path/to/dirsearch/dirsearch.py -u ' alias dirsearch-one=\". (cat domains | awk '{print \\\"dirsearch \\\"\\$1 \\\" -e \\\"}')\" alias openredirect=\". (cat domains | awk '{print \\\"dirsearch \\\"\\$1 \\\" -w /path/to/dirsearch/db/open_redirect_wordlist.txt -e \\\"}')\" Steps to take when approaching a target 1) Verify target\u2019s scope (*.example.com) 2) Run Sublist3r on example.com and output all findings to a file called output: $ sublist3r example.com -o output ... $ cat output foo.example.com bar.example.com 3) Check which domains resolve Use httprobe $ cat output | httprobe | tee -a domains 4) Run webscreenshot on the domains file $ python webscreenshot.py -i domains output example 5) Run dirsearch on the domains file $ dirsearch-one 6) Check for open redirects using dirsearch on the domains file $ openredirect 3. Port Scan a) Masscan - https://github.com/robertdavidgraham/masscan b) Aquatone - Use aquatone to scan the subdomains and then use it for scanning the ports you have options to Scan ports like common/large/huge Scan each individual IP address associated with their subdomains and having the output saved to a file Look for any services running on unusual ports or any service running on default ports which could be vulnerable (FTP, SSH, etc). Look for the version info on services running in order to determine whether anything is outdated and potentially vulnerable 4. Extracting vhosts Tools: https://pentest-tools.com/information-gathering/find-virtual-hosts https://github.com/jobertabma/virtual-host-discovery Screenshot Tools : Planing to Move faster try https://github.com/ChrisTruncer/EyeWitness httpscreenshot https://github.com/breenmachine/httpscreenshot/ Look at the headers to see which security options are in place, for example looking for presence of X-XSS-Protection: or X-Frame-Options: deny. Knowing what security measures are in place means you know your limitations. Look out for WAFs, you can use WafW00f for that https://github.com/sandrogauci/wafw00f Wordlists: https://github.com/danielmiessler/SecLists 5. Directory Bruteforcing a) Dirbuster b) gobuster 3) Burp Intruder 4) Burp Scanner Use robots.txt to determine the directories. Also spider the host for API endpoints. Use Wappalyzer to check CMS or Builtwith or Retire.js or Ghostery 5. Extracting s3 Buckets https://github.com/yasinS/sandcastle https://digi.ninja/projects/bucket_finder.php Keep taking notes Notes typically contains: Whois Information Subdomains Dir info S3 Buckets social accounts API Endpoints emails Vhosts Backend IP address Open Ports / Services running Service version info (if applicable) server banners directory listings presence security headers WAF (+ WAF type) Dig in to website, check each request and response and analysis that, try to understand their infrastructure such as how they\u2019re handling sessions/authentication, what type of CSRF protection they have (if any). Use negative testing to through the error, this Error information is very helpful for me to finding internal paths of the website. Give time to understand the flow of the application to get a better idea of what type of vulnerabilities to look for. Start to dig into using scripts for wordlist bruteforcing endpoints. This can help with finding new directories or folders that you may not have been able to find just using the website. This tends to be private admin panels, source repositories they forgot to remove such as /.git/ folders, or test/debug scripts. After that check each form of the website then try to push client side attacks. Use multiple payloads to bypass client side filters. Popular Google Dorks Use(finding Bug Bounty Websites) site:.eu responsible disclosure inurl:index.php?id= site:.nl bug bounty \u201cindex of\u201d inurl:wp-content/ (Identify Wordpress Website) inurl:\u201dq=user/password\u201d (for finding drupal cms ) Payloads for Hunting Payloads All The Things- https://github.com/swisskyrepo/PayloadsAllTheThings/ XSS Payloads- http://www.xss-payloads.com/ XSS Payloads - https://github.com/Pgaijin66/XSS-Payloads/blob/master/payload.txt SQL Injection Payloads - https://github.com/trietptm/SQL-Injection-Payloads Google-Dorks Payloads - https://gist.github.com/clarketm/919457847cece7ce40323dc217623054 Approach a Target Start early. As soon as a program is launched, start hunting immediately, if you can. Once you start hunting, take a particular functionality/workflow in the application and start digging deep into it. I have stopped caring about low hanging fruits or surface bugs. There is no point focussing your efforts on those. So, let\u2019s say an application has a functionality that allows users to send emails to other users. Observe this workflow/requests via a proxy tool such as Burp. Burp is pretty much the only tool I use for web app pentesting. Create multiple accounts because you would want to test the emails being sent from one user to another. If you haven\u2019t been provided multiple accounts, ask for it. Till date, I have not been refused a second account whenever I have asked for it. Now, if you are slightly experienced, after a few minutes of tinkering with this workflow, you will get a feeling whether it might have something interesting going on or not. This point is difficult to explain. It will come with practice. If the above is true, start fuzzing, breaking the application workflow, inserting random IDs, values, etc. wherever possible. 80% of the time, you will end up noticing weird behavior. The weird behavior doesn\u2019t necessarily mean you have found a bug that is worth reporting. It probably means you have a good chance so you should keep digging into it more. There is some research that might be required as well. Let\u2019s say you found that a particular version of an email server is being used that is outdated. Look on the internet for known vulnerabilities against it. You might encounter a known CVE with a known exploit. Try that exploit and see what happens (provided you are operating under the terms and conditions of the bug bounty). There might be special tools that are required. Explore into that, if possible. Remember, Burp is a swiss army knife but you might have to use certain specific tools in certain cases. Always, be aware of that. After spending a few hours on this, if you think you have exhausted all your options and are not getting anything meaningful out of it, stop and move on. Getting hung up on something is the biggest motivation killer but that doesn\u2019t mean you are giving up. Get back to it later if something else comes up. Make a note of it. Something that has worked for me is bounds checking on parameters, pick a parameter that has an obvious effect on the flow of the application. For example, if a field takes a number (lets call it ID for lulz). What happens if: -you put in a minus number? -you increment or decrement the number? -you put in a really large number? -you put in a string or symbol characters? -you try traverse a directory with \u2026/ -you put in XSS vectors? -you put in SQLI vectors? -you put in non-ascii characters? -you mess with the variable type such as casting a string to an array -you use null characters or no value I would then see if I can draw any conclusions from the outcomes of these tests, -see if I can understand what is happening based on an error -is anything broken or exposed -can this action affect other things in the app. Focus on site functionality that has been redesigned or changed since a previous version of the target. Sometimes, having seen/used a bounty product before, you will notice right away any new functionality. Other times you will read the bounty brief a few times and realize that they are giving you a map. Developers often point out the areas they think they are weak in. They/us want you to succeed. A visual example would be new search functionality, role based access, etc. A bounty brief example would be reading a brief and noticing a lot of pointed references to the API or a particular page/function in the site. If the scope allows (and you have the skillset) test the crap out of the mobile apps. While client side bugs continue to grow less severe, the API\u2019s/web-endpoints the mobile apps talk to often touch partsof the application you wouldn\u2019t have seen in a regular workflow. This is not to say client side bugs are not reportable, they just become low severity issues as the mobile OS\u2019s raise the bar security-wise. So after you have a thorough \u201cfeeling\u201d for the site you need to mentally or physically keep a record of workflows in the application. You need to start asking yourself questions like these: Does the page functionality display something to the users? (XSS,Content Spoofing, etc) Does the page look like it might need to call on stored data? (Injections of all type, Indirect object references, client side storage) Does it (or can it) interact with the server file system? (Fileupload vulns, LFI, etc) Is it a function worthy of securing? (CSRF, Mixed-mode) Is this function a privileged one? (logic flaws, IDORs, priv escalations) ++ Where is input accepted and potentially displayed to the user? What endpoints save data? Any file upload functionality? What type of authentication is used? Tools https://github.com/ehsahil/recon-my-way Read every JS Sometimes, Javascript files contain sensitive information including various secrets or hardcoded tokens. It\u2019s always worth to examine JS files manually. Find following things in Javascript. - AWS or Other services Access keys - AWS S3 buckets or other data storage buckets with read/write permissions. - Open backup sql database endpoints - Open endpoints of internal services. JSParser - extracting links from JS file LinkFinder(https://github.com/GerbenJavado/LinkFinder) - extracting endpoints from JS files Wordlists amass aquatone More subdomains: censys-enumeration - https://github.com/0xbharath/censys-enumeration censys-subdomain-finder shodan Viewdns.info - Reverse whois lookup - if you know the \"email id \" in the registrar of a domain and you want to check what other domains are registered with the same email id you can use this site. Get email address using - whois \\ domain.com> $ whois domain.com If the company is not using Domain Privacy Service, you will find the host-masters email address then use that email to find other domains registered on same email address using Reverse Whois Lookup. Targets Registered legal name also can be used. altdns - https://github.com/infosec-au/altdns domains-from-csp - https://github.com/0xbharath/domains-from-csp domain-profiler - https://github.com/jpf/domain-profiler Certspotter - https://certspotter.com/api/v0/certs?domain=hackerone.com Crt.sh - https://crt.sh/?q=%25domain.com VHostScan - https://github.com/codingo/VHostScan dirsearch knockpy - Knockpy is a python tool designed to enumerate subdomains on a target domain through a wordlist. It is designed to scan for DNS zone transfer and to try to bypass the wildcard DNS record automatically if it is enabled. Example from @ehsahil:Why I use knockpy Initially? It provides me with a quick overlook of the subdomains with a response code. Once, I found a subdomain takeover bug within 2 mins. I ran knockpy on an old program\u2019s in-scope asset with almost 150 bugs resolved on HackerOne. Quickly saw 404 page pointed to AWS S3 bucket and bucket were available to create. Hence, with no delay, I created the new AWS S3 bucket and uploaded a text file with the encoded filename and reported the bug. lazys3(https://github.com/nahamsec/lazys3) - LazyS3 is an another tool which I use almost frequently to find the staging, sandboxed, dev and production buckets. lazyshot masscan init s3-buckets-finder - https://digi.ninja/projects/bucket_finder.php subfinder subresolve AWS-CLI - AWS CLI is useful for verifying or testing the permissions of the AWS S3 buckets, Creating Buckets and Read other buckets data. AWS Account needed to use CLI Github For Recon Github is extremely helpful in finding Sensitive information regarding the targets. Access-keys, password, open endings, s3 buckets, backup files, etc. can be found on public GitHub repositories. subdomain.rb - Subdomains By Subfinder, Subdomains By Amass, Subdomains BY Censys, Subdomains by Knockpy, Subdomains By Aquatone-discover, Subdomains By Aquatone-takeover First find sumdoomains, sort it and pass the unique sorted domain file to recon.rb recon.rb - host, nmap, aws, dirsearch, gobuster * waybackurl.py* - Searching for the targets webpages in waybackmachine, the following things can be found. Old and abandoned JS files. Old API endpoints. Abandoned CDN\u2019s Endpoints. Abandoned Subdomains. Dev staging endpoint with juicy info in source code comments. If you are getting 403 on a page, you can also search that 403 pages of targets in way back machine sometimes, you will find them open with helpful information. lazyrecon : https://github.com/nahamsec/lazyrecon Create a dated folder with recon notes Grab subdomains using: Sublist3r, certspotter and cert.sh Dns bruteforcing using massdns Find any CNAME records pointing to unused cloud services like aws Probe for live hosts over ports 80/443 Grab a screenshots of responsive hosts Scrape wayback for data: Extract javascript files Build custom parameter wordlist, ready to be loaded later into Burp intruder or any other tool Extract any urls with .jsp, .php or .aspx and store them for further inspection Perform nmap on specific ports Get dns information about every subdomain Perform dirsearch for all subdomains Parameth - parameth.py -u example.com/login.php -t 30 -o output.txt Generate a HTML report with output from the tools above Jason Haddix (https://twitter.com/jhaddix/status/972926512595746816?lang=en) The lost art of LINKED target discovery w/ Burp Suite: 1) Turn off passive scanning 2) Set forms auto to submit 3) Set scope to advanced control and use string of target name (not a normal FQDN) 4) Walk+browse, then spider all hosts recursively! 5) Profit (more targets)! My Methodology: Subdomain Enumeration Httprobe Subdomains to get resolved and working subdomains Visuan Recon Web Security Mindmap Recon Tree by @nahamsec Practical Recon by Bharath Kumar Enumerating Domains a. Vertical domain correlation (all the subdomain of a domain) (maps.google.com) b. Horizontal domain corerelation ( like google.com, google.cz, youtube.com) Google Dork to use subdomain (site: helpful for finding vertical domain, ip: helpful for horizontal domain corerelation) Always make shell function to automate things like this simple script: find-subdomain-vt() { curl -s https://www.virustotal.com/ui/domains/$1/subdomains\\?limit\\=$2 } Certificate Transparency you can get domain names, subdomain names email address in a certificate you can use these websites: crt.sh - example %eff.com certspotter - vertical and horizontal corelation censys.io - It stores SSL certificates, good source of domains and email address developers.facebook.com/tools/ct/ -- this is interesting, as FB allows you to provide the domain name and email address so if there is any new subdomain comes, you will get notification on your email. google.com/transparencyreport/https/ct/ Downside of CT for recon is that there is no way to remove the certificate entry so if the subdomain doesn't even exist, certificate for that still exist. Use massdns along with CT logs to identify resolvable domains or use httprobe When setting up some CMSs like Wordpress, or Joomla, there is a window of time when there is no authentication so , if you find subomain running HTTPS and you check the CT logs as soon as certificate is issued, this can be easily exploited. All the scripts are present at - https://github.com/appsecco/the-art-of-subdomain-enumeration Content Security Policy Content-Security-Policy header allows us to create a whitelist of sources of trusted content, and instructs the browser to only execute or render resources from those domains(sources). Script : https://github.com/ehsahil/recon-my-way/blob/master/domains-from-csp/csp_parser.py 2. DNS: Using SPF record of DNS - https://github.com/0xbharath/assets-from-spf/blob/master/assets_from_spf.py you can give --asn to give you which belongs the IP block. curl -s http://ip-api.com/json/192.30.253.113 | jq -r .as AS36459 GitHub, Inc. The ASN numbers found can be used to find netblocks of the domain. We can use advanced WHOIS queries to find all the IP ranges that belong to an ASN $ whois -h whois.radb.net -- '-i origin AS36459' | grep -Eo \"([0-9.]+){4}/[0-9]+\" | uniq There is an Nmap script to find IP ranges that belong to an ASN that https://nmap.org/nsedoc/scripts/targets-asn.html $ nmap --script targets-asn --script-args targets-asn.asn=17012 paypal.txt you can use dig $ dig AXFR @\\ nameserver> \\ domain_name> 3. Hunting for cloud Services site:s3.amazonaws.com file:pdf site:s3.amazonaws.com password or site:s3.amazonaws.com + inurl:company_name tools: slurp, awsbucketdump, spacefinder 4. Github Repos for Recon edoverflow technique search for token,key, secret, password Nahmsec 1. Asset Discovery Buteforce Find different environments(.dev, .corp, .stage, uat etc) Brute force again different permutations, different environments Find the pattern how company is making subdomains, and use that pattern to create more permustation like dashboard-dev or dashboard.dev Tools Used: Sublist3r enumall massdns altdns brutesubs dns-parallel-prober dnscan knockpy HostileSubButeforcer Certificate Transparency This is the most important steps, because the subdomains names that you find here, you cannot find from other bruteforce tools because yourwordlist does not have pattern that are available in all the subdomains or does not have keyword like gateway or payment which are part of subdomain. Censys Look for SSL certificate: 443.https.tls.certificate.parsed.extensions.subject_alt_name.dns_names:snapchat.com Shodan Ports:8443, 8080 Title: \"Dashboard[Jenkins]\" Product: Tomcat Hostname: example.com Org: google ssl:Google Use Certspotter Use Crt.sh Acquisitions Check on crunchbase which company acquired what other companies. So now you get all the subdomains, but now what ??? Run httprobe on them to find which one are resolving. Content Discovery Port Scan Screenshot open ports Look for interesting files or directories, Check robots.txt, Bruteforce (dirbuster, gograbber, gobuster, dirsearch) example: you see an open port on 8443 Directory brute force /admin/ return 403 You bruteforce for more files/direcotries on /admin/ and let's say /admin/users.php return 200 Repeat on other domain, ports, folders etc Amazon S3 bucket tools: lazys3, bucketeer Github Recon Tools: - gitrob - truffleHog - git-secrets - repo-supervisors - git-all-secrets Do it manually \"company.com\" \"dev\" \"dev.company.com\" \"company.com\" API_key \"company.com\" password \"api.company.com\" authorization others Wayback Machine Bug Bounty Methodology Discovery: Note: If you bruteforce directory, and get 401 status code(unauthorized), then keep bruteforcing inside that direcotory. Things to check: Visit the search, registration, contact, password reset, and comment forms and hit them with your polyglot strings Scan those specific functions with Burp\u2019s built-in scanner Check your cookie, log out, check cookie, log in, check cookie. Submit old cookie, see if access. Perform user enumeration checks on login, registration, and password reset. Do a reset and see if; the password comes plaintext, uses a URL based token, is predictable, can be used multiple times, or logs you in automatically Find numeric account identifiers anywhere in URLs and rotate them for context change Find the security-sensitive function(s) or files and see if vulnerable to non-auth browsing (idors), lower-auth browsing, CSRF, CSRF protection bypass, and see if they can be done over HTTP. Directory brute for top short list on SecLists Check upload functions for alternate file types that can execute code (xss or php/etc/etc) Parameter Bruteforcing Parameth Use Parameth with portswiggerbackslash powered scanner Identify IPs and main TLDs from Bug Bounty Scope (ASNs, Reverse whois) Subdomain Enumeration Domain bruteforcing, httprobe, massdns Port Scan using masscan Screenshot Platform Identification(Wappalyzerm builtwith) Content Discovery(Gobuster, dirbuster) Parameter discovery(Parameth, Arjun) Blind XSS - XSSHunter Commix for php code injection Bug Hunter's Methodology V3 Discovering IP Space: Autonomous System Number (ASN) - http://bgp.he.net - check for example tesla.com and checkin Prefixes V4 to get the IP range Reverse whois http://reverse.report Shodan org:\"Tesla Motors\" Discovery New Targets Linked Discovery","title":"Recon Process"},{"location":"BugBountyRecon/#bug-bounty-hunting-tip-1-always-read-the-source-code","text":"","title":"Bug Bounty Hunting Tip #1- Always read the Source Code"},{"location":"BugBountyRecon/#1-sumdomain-enumeration","text":"","title":"1. Sumdomain Enumeration"},{"location":"BugBountyRecon/#enumerate-subdomains","text":"Web Tools: https://pentest-tools.com/ https://virustotal.com/ https://www.shodan.io/ https://crt.sh/?q=%25taregt.com https://dnsdumpster.com/ https://censys.io http://dnsgoodies.com Tools: https://bitbucket.org/LaNMaSteR53/recon-ng https://github.com/michenriksen/aquatone https://github.com/aboul3la/Sublist3r https://github.com/rbsec/dnscan https://github.com/Cleveridge/cleveridge-subdomain-scanner","title":"Enumerate Subdomains"},{"location":"BugBountyRecon/#screenshot-tools","text":"webscreenshot Make sure to install PhantomJS too. $ git clone https://github.com/maaaaz/webscreenshot.git Once this is done, we use a tool called epg-prep (https://www.npmjs.com/package/epg-prep) to create thumbnails to do so, simply run: epg-prep uber.com This will allow us to view the created pictures using express-photo-gallery. In a final step, use the express-gallery-script from the bottom of this blogpost and save it as yourname.js. All you need to do is to change the folder name inside the script: app.use('/photos', Gallery('uber.com', options)); the folder name in this case is set uber.com but depending on which target you look at it may be different. Once you\u2019ve done that you can simply run the script using node yourname.js. This will create a webserver listening on Port 3000 with an endpoint called /photos. So to access this you simply type: http://yourserverip:3000/photos to get a nice overview of the subdomains you have enumerated System Tools apt update apt upgrade curl -sL https://deb.nodesource.com/setup_6.x | sudo -E bash - apt install -y git wget python python-pip phantomjs xvfb screen slurm gem phantomjs imagemagick graphicsmagick nodejs Requirements for WebScreenshot pip install webscreenshot pip install selenium Requirements for express-photo-gallery sudo npm install -g npm npm install express-photo-gallery npm install express npm install -g epg-prep Requirements for Aquatone git clone https://github.com/michenriksen/aquatone.git cd aquatone/ gem install aquatone express-photo-gallery Script JavaScript var express = require('express'); var app = express(); var Gallery = require('express-photo-gallery'); var options = { title: 'My Awesome Photo Gallery' }; app.use('/photos', Gallery('uber.com', options)); app.listen(3000);","title":"Screenshot Tools"},{"location":"BugBountyRecon/#sublist3r","text":"$ git clone https://github.com/aboul3la/Sublist3r.git $ cd Sublist3r $ sudo pip install -r requirements.txt alias sublist3r='python /path/to/Sublist3r/sublist3r.py -d ' alias sublist3r-one=\". (cat domains | awk '{print \\\"sublist3r \\\"$1 \\\" -o \\\" $1 \\\".txt\\\"}')\"","title":"Sublist3r"},{"location":"BugBountyRecon/#dirsearch","text":"$ git clone https://github.com/maurosoria/dirsearch.git $ cd dirsearch/db $ wget https://gist.githubusercontent.com/EdOverflow/c4d6d8c43b315546892aa5dab67fdd6c/raw/7dc210b17d7742b46de340b824a0caa0f25cf3cc/open_redirect_wordlist.txt alias dirsearch='python3 /path/to/dirsearch/dirsearch.py -u ' alias dirsearch-one=\". (cat domains | awk '{print \\\"dirsearch \\\"\\$1 \\\" -e \\\"}')\" alias openredirect=\". (cat domains | awk '{print \\\"dirsearch \\\"\\$1 \\\" -w /path/to/dirsearch/db/open_redirect_wordlist.txt -e \\\"}')\"","title":"dirsearch"},{"location":"BugBountyRecon/#steps-to-take-when-approaching-a-target","text":"1) Verify target\u2019s scope (*.example.com) 2) Run Sublist3r on example.com and output all findings to a file called output: $ sublist3r example.com -o output ... $ cat output foo.example.com bar.example.com 3) Check which domains resolve Use httprobe $ cat output | httprobe | tee -a domains 4) Run webscreenshot on the domains file $ python webscreenshot.py -i domains output example 5) Run dirsearch on the domains file $ dirsearch-one 6) Check for open redirects using dirsearch on the domains file $ openredirect","title":"Steps to take when approaching a target"},{"location":"BugBountyRecon/#3-port-scan","text":"a) Masscan - https://github.com/robertdavidgraham/masscan b) Aquatone - Use aquatone to scan the subdomains and then use it for scanning the ports you have options to Scan ports like common/large/huge Scan each individual IP address associated with their subdomains and having the output saved to a file Look for any services running on unusual ports or any service running on default ports which could be vulnerable (FTP, SSH, etc). Look for the version info on services running in order to determine whether anything is outdated and potentially vulnerable","title":"3. Port Scan"},{"location":"BugBountyRecon/#4-extracting-vhosts","text":"Tools: https://pentest-tools.com/information-gathering/find-virtual-hosts https://github.com/jobertabma/virtual-host-discovery Screenshot Tools : Planing to Move faster try https://github.com/ChrisTruncer/EyeWitness httpscreenshot https://github.com/breenmachine/httpscreenshot/ Look at the headers to see which security options are in place, for example looking for presence of X-XSS-Protection: or X-Frame-Options: deny. Knowing what security measures are in place means you know your limitations. Look out for WAFs, you can use WafW00f for that https://github.com/sandrogauci/wafw00f Wordlists: https://github.com/danielmiessler/SecLists","title":"4. Extracting vhosts"},{"location":"BugBountyRecon/#5-directory-bruteforcing","text":"a) Dirbuster b) gobuster 3) Burp Intruder 4) Burp Scanner Use robots.txt to determine the directories. Also spider the host for API endpoints. Use Wappalyzer to check CMS or Builtwith or Retire.js or Ghostery","title":"5. Directory Bruteforcing"},{"location":"BugBountyRecon/#5-extracting-s3-buckets","text":"https://github.com/yasinS/sandcastle https://digi.ninja/projects/bucket_finder.php","title":"5. Extracting s3 Buckets"},{"location":"BugBountyRecon/#keep-taking-notes","text":"Notes typically contains: Whois Information Subdomains Dir info S3 Buckets social accounts API Endpoints emails Vhosts Backend IP address Open Ports / Services running Service version info (if applicable) server banners directory listings presence security headers WAF (+ WAF type) Dig in to website, check each request and response and analysis that, try to understand their infrastructure such as how they\u2019re handling sessions/authentication, what type of CSRF protection they have (if any). Use negative testing to through the error, this Error information is very helpful for me to finding internal paths of the website. Give time to understand the flow of the application to get a better idea of what type of vulnerabilities to look for. Start to dig into using scripts for wordlist bruteforcing endpoints. This can help with finding new directories or folders that you may not have been able to find just using the website. This tends to be private admin panels, source repositories they forgot to remove such as /.git/ folders, or test/debug scripts. After that check each form of the website then try to push client side attacks. Use multiple payloads to bypass client side filters.","title":"Keep taking notes"},{"location":"BugBountyRecon/#popular-google-dorks-usefinding-bug-bounty-websites","text":"site:.eu responsible disclosure inurl:index.php?id= site:.nl bug bounty \u201cindex of\u201d inurl:wp-content/ (Identify Wordpress Website) inurl:\u201dq=user/password\u201d (for finding drupal cms )","title":"Popular Google Dorks Use(finding Bug Bounty Websites)"},{"location":"BugBountyRecon/#payloads-for-hunting","text":"Payloads All The Things- https://github.com/swisskyrepo/PayloadsAllTheThings/ XSS Payloads- http://www.xss-payloads.com/ XSS Payloads - https://github.com/Pgaijin66/XSS-Payloads/blob/master/payload.txt SQL Injection Payloads - https://github.com/trietptm/SQL-Injection-Payloads Google-Dorks Payloads - https://gist.github.com/clarketm/919457847cece7ce40323dc217623054","title":"Payloads for Hunting"},{"location":"BugBountyRecon/#approach-a-target","text":"Start early. As soon as a program is launched, start hunting immediately, if you can. Once you start hunting, take a particular functionality/workflow in the application and start digging deep into it. I have stopped caring about low hanging fruits or surface bugs. There is no point focussing your efforts on those. So, let\u2019s say an application has a functionality that allows users to send emails to other users. Observe this workflow/requests via a proxy tool such as Burp. Burp is pretty much the only tool I use for web app pentesting. Create multiple accounts because you would want to test the emails being sent from one user to another. If you haven\u2019t been provided multiple accounts, ask for it. Till date, I have not been refused a second account whenever I have asked for it. Now, if you are slightly experienced, after a few minutes of tinkering with this workflow, you will get a feeling whether it might have something interesting going on or not. This point is difficult to explain. It will come with practice. If the above is true, start fuzzing, breaking the application workflow, inserting random IDs, values, etc. wherever possible. 80% of the time, you will end up noticing weird behavior. The weird behavior doesn\u2019t necessarily mean you have found a bug that is worth reporting. It probably means you have a good chance so you should keep digging into it more. There is some research that might be required as well. Let\u2019s say you found that a particular version of an email server is being used that is outdated. Look on the internet for known vulnerabilities against it. You might encounter a known CVE with a known exploit. Try that exploit and see what happens (provided you are operating under the terms and conditions of the bug bounty). There might be special tools that are required. Explore into that, if possible. Remember, Burp is a swiss army knife but you might have to use certain specific tools in certain cases. Always, be aware of that. After spending a few hours on this, if you think you have exhausted all your options and are not getting anything meaningful out of it, stop and move on. Getting hung up on something is the biggest motivation killer but that doesn\u2019t mean you are giving up. Get back to it later if something else comes up. Make a note of it. Something that has worked for me is bounds checking on parameters, pick a parameter that has an obvious effect on the flow of the application. For example, if a field takes a number (lets call it ID for lulz). What happens if: -you put in a minus number? -you increment or decrement the number? -you put in a really large number? -you put in a string or symbol characters? -you try traverse a directory with \u2026/ -you put in XSS vectors? -you put in SQLI vectors? -you put in non-ascii characters? -you mess with the variable type such as casting a string to an array -you use null characters or no value I would then see if I can draw any conclusions from the outcomes of these tests, -see if I can understand what is happening based on an error -is anything broken or exposed -can this action affect other things in the app. Focus on site functionality that has been redesigned or changed since a previous version of the target. Sometimes, having seen/used a bounty product before, you will notice right away any new functionality. Other times you will read the bounty brief a few times and realize that they are giving you a map. Developers often point out the areas they think they are weak in. They/us want you to succeed. A visual example would be new search functionality, role based access, etc. A bounty brief example would be reading a brief and noticing a lot of pointed references to the API or a particular page/function in the site. If the scope allows (and you have the skillset) test the crap out of the mobile apps. While client side bugs continue to grow less severe, the API\u2019s/web-endpoints the mobile apps talk to often touch partsof the application you wouldn\u2019t have seen in a regular workflow. This is not to say client side bugs are not reportable, they just become low severity issues as the mobile OS\u2019s raise the bar security-wise. So after you have a thorough \u201cfeeling\u201d for the site you need to mentally or physically keep a record of workflows in the application. You need to start asking yourself questions like these: Does the page functionality display something to the users? (XSS,Content Spoofing, etc) Does the page look like it might need to call on stored data? (Injections of all type, Indirect object references, client side storage) Does it (or can it) interact with the server file system? (Fileupload vulns, LFI, etc) Is it a function worthy of securing? (CSRF, Mixed-mode) Is this function a privileged one? (logic flaws, IDORs, priv escalations) ++ Where is input accepted and potentially displayed to the user? What endpoints save data? Any file upload functionality? What type of authentication is used?","title":"Approach a Target"},{"location":"BugBountyRecon/#tools","text":"https://github.com/ehsahil/recon-my-way Read every JS Sometimes, Javascript files contain sensitive information including various secrets or hardcoded tokens. It\u2019s always worth to examine JS files manually. Find following things in Javascript. - AWS or Other services Access keys - AWS S3 buckets or other data storage buckets with read/write permissions. - Open backup sql database endpoints - Open endpoints of internal services. JSParser - extracting links from JS file LinkFinder(https://github.com/GerbenJavado/LinkFinder) - extracting endpoints from JS files Wordlists amass aquatone More subdomains: censys-enumeration - https://github.com/0xbharath/censys-enumeration censys-subdomain-finder shodan Viewdns.info - Reverse whois lookup - if you know the \"email id \" in the registrar of a domain and you want to check what other domains are registered with the same email id you can use this site. Get email address using - whois \\ domain.com> $ whois domain.com If the company is not using Domain Privacy Service, you will find the host-masters email address then use that email to find other domains registered on same email address using Reverse Whois Lookup. Targets Registered legal name also can be used. altdns - https://github.com/infosec-au/altdns domains-from-csp - https://github.com/0xbharath/domains-from-csp domain-profiler - https://github.com/jpf/domain-profiler Certspotter - https://certspotter.com/api/v0/certs?domain=hackerone.com Crt.sh - https://crt.sh/?q=%25domain.com VHostScan - https://github.com/codingo/VHostScan dirsearch knockpy - Knockpy is a python tool designed to enumerate subdomains on a target domain through a wordlist. It is designed to scan for DNS zone transfer and to try to bypass the wildcard DNS record automatically if it is enabled. Example from @ehsahil:Why I use knockpy Initially? It provides me with a quick overlook of the subdomains with a response code. Once, I found a subdomain takeover bug within 2 mins. I ran knockpy on an old program\u2019s in-scope asset with almost 150 bugs resolved on HackerOne. Quickly saw 404 page pointed to AWS S3 bucket and bucket were available to create. Hence, with no delay, I created the new AWS S3 bucket and uploaded a text file with the encoded filename and reported the bug. lazys3(https://github.com/nahamsec/lazys3) - LazyS3 is an another tool which I use almost frequently to find the staging, sandboxed, dev and production buckets. lazyshot masscan init s3-buckets-finder - https://digi.ninja/projects/bucket_finder.php subfinder subresolve AWS-CLI - AWS CLI is useful for verifying or testing the permissions of the AWS S3 buckets, Creating Buckets and Read other buckets data. AWS Account needed to use CLI Github For Recon Github is extremely helpful in finding Sensitive information regarding the targets. Access-keys, password, open endings, s3 buckets, backup files, etc. can be found on public GitHub repositories. subdomain.rb - Subdomains By Subfinder, Subdomains By Amass, Subdomains BY Censys, Subdomains by Knockpy, Subdomains By Aquatone-discover, Subdomains By Aquatone-takeover First find sumdoomains, sort it and pass the unique sorted domain file to recon.rb recon.rb - host, nmap, aws, dirsearch, gobuster * waybackurl.py* - Searching for the targets webpages in waybackmachine, the following things can be found. Old and abandoned JS files. Old API endpoints. Abandoned CDN\u2019s Endpoints. Abandoned Subdomains. Dev staging endpoint with juicy info in source code comments. If you are getting 403 on a page, you can also search that 403 pages of targets in way back machine sometimes, you will find them open with helpful information. lazyrecon : https://github.com/nahamsec/lazyrecon Create a dated folder with recon notes Grab subdomains using: Sublist3r, certspotter and cert.sh Dns bruteforcing using massdns Find any CNAME records pointing to unused cloud services like aws Probe for live hosts over ports 80/443 Grab a screenshots of responsive hosts Scrape wayback for data: Extract javascript files Build custom parameter wordlist, ready to be loaded later into Burp intruder or any other tool Extract any urls with .jsp, .php or .aspx and store them for further inspection Perform nmap on specific ports Get dns information about every subdomain Perform dirsearch for all subdomains Parameth - parameth.py -u example.com/login.php -t 30 -o output.txt Generate a HTML report with output from the tools above Jason Haddix (https://twitter.com/jhaddix/status/972926512595746816?lang=en) The lost art of LINKED target discovery w/ Burp Suite: 1) Turn off passive scanning 2) Set forms auto to submit 3) Set scope to advanced control and use string of target name (not a normal FQDN) 4) Walk+browse, then spider all hosts recursively! 5) Profit (more targets)! My Methodology: Subdomain Enumeration Httprobe Subdomains to get resolved and working subdomains Visuan Recon","title":"Tools"},{"location":"BugBountyRecon/#web-security-mindmap","text":"","title":"Web Security Mindmap"},{"location":"BugBountyRecon/#recon-tree-by-nahamsec","text":"","title":"Recon Tree by @nahamsec"},{"location":"BugBountyRecon/#practical-recon-by-bharath-kumar","text":"Enumerating Domains a. Vertical domain correlation (all the subdomain of a domain) (maps.google.com) b. Horizontal domain corerelation ( like google.com, google.cz, youtube.com) Google Dork to use subdomain (site: helpful for finding vertical domain, ip: helpful for horizontal domain corerelation) Always make shell function to automate things like this simple script: find-subdomain-vt() { curl -s https://www.virustotal.com/ui/domains/$1/subdomains\\?limit\\=$2 } Certificate Transparency you can get domain names, subdomain names email address in a certificate you can use these websites: crt.sh - example %eff.com certspotter - vertical and horizontal corelation censys.io - It stores SSL certificates, good source of domains and email address developers.facebook.com/tools/ct/ -- this is interesting, as FB allows you to provide the domain name and email address so if there is any new subdomain comes, you will get notification on your email. google.com/transparencyreport/https/ct/ Downside of CT for recon is that there is no way to remove the certificate entry so if the subdomain doesn't even exist, certificate for that still exist. Use massdns along with CT logs to identify resolvable domains or use httprobe When setting up some CMSs like Wordpress, or Joomla, there is a window of time when there is no authentication so , if you find subomain running HTTPS and you check the CT logs as soon as certificate is issued, this can be easily exploited. All the scripts are present at - https://github.com/appsecco/the-art-of-subdomain-enumeration Content Security Policy Content-Security-Policy header allows us to create a whitelist of sources of trusted content, and instructs the browser to only execute or render resources from those domains(sources). Script : https://github.com/ehsahil/recon-my-way/blob/master/domains-from-csp/csp_parser.py 2. DNS: Using SPF record of DNS - https://github.com/0xbharath/assets-from-spf/blob/master/assets_from_spf.py you can give --asn to give you which belongs the IP block. curl -s http://ip-api.com/json/192.30.253.113 | jq -r .as AS36459 GitHub, Inc. The ASN numbers found can be used to find netblocks of the domain. We can use advanced WHOIS queries to find all the IP ranges that belong to an ASN $ whois -h whois.radb.net -- '-i origin AS36459' | grep -Eo \"([0-9.]+){4}/[0-9]+\" | uniq There is an Nmap script to find IP ranges that belong to an ASN that https://nmap.org/nsedoc/scripts/targets-asn.html $ nmap --script targets-asn --script-args targets-asn.asn=17012 paypal.txt you can use dig $ dig AXFR @\\ nameserver> \\ domain_name> 3. Hunting for cloud Services site:s3.amazonaws.com file:pdf site:s3.amazonaws.com password or site:s3.amazonaws.com + inurl:company_name tools: slurp, awsbucketdump, spacefinder 4. Github Repos for Recon edoverflow technique search for token,key, secret, password Nahmsec 1. Asset Discovery Buteforce Find different environments(.dev, .corp, .stage, uat etc) Brute force again different permutations, different environments Find the pattern how company is making subdomains, and use that pattern to create more permustation like dashboard-dev or dashboard.dev Tools Used: Sublist3r enumall massdns altdns brutesubs dns-parallel-prober dnscan knockpy HostileSubButeforcer Certificate Transparency This is the most important steps, because the subdomains names that you find here, you cannot find from other bruteforce tools because yourwordlist does not have pattern that are available in all the subdomains or does not have keyword like gateway or payment which are part of subdomain. Censys Look for SSL certificate: 443.https.tls.certificate.parsed.extensions.subject_alt_name.dns_names:snapchat.com Shodan Ports:8443, 8080 Title: \"Dashboard[Jenkins]\" Product: Tomcat Hostname: example.com Org: google ssl:Google Use Certspotter Use Crt.sh Acquisitions Check on crunchbase which company acquired what other companies. So now you get all the subdomains, but now what ??? Run httprobe on them to find which one are resolving. Content Discovery Port Scan Screenshot open ports Look for interesting files or directories, Check robots.txt, Bruteforce (dirbuster, gograbber, gobuster, dirsearch) example: you see an open port on 8443 Directory brute force /admin/ return 403 You bruteforce for more files/direcotries on /admin/ and let's say /admin/users.php return 200 Repeat on other domain, ports, folders etc Amazon S3 bucket tools: lazys3, bucketeer Github Recon Tools: - gitrob - truffleHog - git-secrets - repo-supervisors - git-all-secrets Do it manually \"company.com\" \"dev\" \"dev.company.com\" \"company.com\" API_key \"company.com\" password \"api.company.com\" authorization others Wayback Machine Bug Bounty Methodology Discovery: Note: If you bruteforce directory, and get 401 status code(unauthorized), then keep bruteforcing inside that direcotory. Things to check: Visit the search, registration, contact, password reset, and comment forms and hit them with your polyglot strings Scan those specific functions with Burp\u2019s built-in scanner Check your cookie, log out, check cookie, log in, check cookie. Submit old cookie, see if access. Perform user enumeration checks on login, registration, and password reset. Do a reset and see if; the password comes plaintext, uses a URL based token, is predictable, can be used multiple times, or logs you in automatically Find numeric account identifiers anywhere in URLs and rotate them for context change Find the security-sensitive function(s) or files and see if vulnerable to non-auth browsing (idors), lower-auth browsing, CSRF, CSRF protection bypass, and see if they can be done over HTTP. Directory brute for top short list on SecLists Check upload functions for alternate file types that can execute code (xss or php/etc/etc)","title":"Practical Recon by Bharath Kumar"},{"location":"BugBountyRecon/#parameter-bruteforcing","text":"Parameth Use Parameth with portswiggerbackslash powered scanner Identify IPs and main TLDs from Bug Bounty Scope (ASNs, Reverse whois) Subdomain Enumeration Domain bruteforcing, httprobe, massdns Port Scan using masscan Screenshot Platform Identification(Wappalyzerm builtwith) Content Discovery(Gobuster, dirbuster) Parameter discovery(Parameth, Arjun) Blind XSS - XSSHunter","title":"Parameter Bruteforcing"},{"location":"BugBountyRecon/#commix-for-php-code-injection","text":"Bug Hunter's Methodology V3 Discovering IP Space: Autonomous System Number (ASN) - http://bgp.he.net - check for example tesla.com and checkin Prefixes V4 to get the IP range Reverse whois http://reverse.report Shodan org:\"Tesla Motors\" Discovery New Targets Linked Discovery","title":"Commix for php code injection"},{"location":"Virtual_Lab_Setup/","text":"For running tools required for pentesting, you need to setup your own linux. There are many linux flavour available in market, like Ubuntu, RedHat, OpenSuse but Kali is one of the best for people in infosec as it contains all the tools preinstalled which are required by any pentester. It is always better to install Kali as a guest OS/virtual OS, on top of existing OS like windows, reason is, since kali OS is going to be the source of all attacks what you are going to do against other target(other PCs/Websites), even if your own system got hacked by unethical people it will only effect the vmware OS only, not the original OS on top of which you are running your kali. So, before installing Kali OS, you need to install hypervisor software, there are 2 well supported hypervisor available in market, VMWare and VirtualBox. I personally like working with Vmware, so download and install Vmware from this link - https://www.vmware.com/products/workstation-player.html Once Vmware got installed, download zip file of kali linux from this link - https://www.offensive-security.com/kali-linux-vm-vmware-virtualbox-image-download/ or if you applied for OSCP, you will get a download link of kali from OSCP in a mail. Only difference between these 2 kali version is that the OSCP kali linux contains some extra preinstalled tools which might be needed during OSCP. Unzip above file using 7z software, you can download 7z from here, if you don't have it already: https://www.7-zip.org/download.html Go to the extracted folder and open file having type as Vmware virtual machine configuration(Kali-Linux-2019.1-vm-amd64). Once it get opened it will show prompt like below: Click on \"I Copied It\". Once it boots, type Username: root and Passwords: toor There might be chances that you need to change the network adapter settings. VMware offers 3 options for virtual network connections: bridged, NAT, and host only. You should choose the bridged option, but below is the info on each: - Bridged network: It connects the virtual machine directly to the local network using the same connection as the host system. As far as the local network is concerned, our virtual machine is just another node on the network with its own IP address. - NAT(Network Address Translation): It sets up a private network on the host machine. The private network translates outgoing traffic from the virtual machine to the local network. On the local network, traffic from the virtual machine will appear to come from the host machine\u2019s IP address. - Host-only network: It limits the virtual machine to a local private network on the host. The virtual machine will be able to communicate with other virtual machines in the host-only network as well as the host machine itself, but it will not be able to send or receive any traffic with the local network or the Internet. Chose Bridge Network Connection: ''' Go to player- Machine - Virtual Machine Settings - Bridged option. ''' To learn Linux : https://linuxjourney.com/ https://ryanstutorials.net/linuxtutorial/ if you want to learn kali linux: you can skip wireless part from this video: https://www.youtube.com/watch?v=0uvWRwLs5Zo kali linux book: https://kali.training/downloads/Kali-Linux-Revealed-1st-edition.pdf If you want to understand any command, you can put the command in below website to get help about it: https://www.explainshell.com/","title":"Setting up Kali Linux"},{"location":"idor/","text":"Authorize and Autorepeater burp plugins to find IDOR. Authorize For each request you do, it will send a equal request with something changed.. And this something is usually cookie of the session or any additional header. Example: User A will browse the webapp and we will use authorize to use cookies of user B automatically. Copy all the cookies if user 1, paste in authorize configuration In interception filter, add filters: Ignore spider request Scope items only URL Not Contains(regex): .Js|css|png|jpg|jpeg|gif|woff|BMP|ICO$ Start Authorize Browse Web using user1 and authorize will capture the request, and send the request again with modified cookies of user 2 Now you can click on each indiviual request to check modified response, if response is 403 forbidden that means user2 is not authorize to browse that webpage. 403 == Enforced == No Go So go through the webapp, browsing everything, changing profile and saving adn authorize will show if it is enforced or not. if it is not, try reading the response to check if it is vulnerable or not. Autorepeater Autorepeater is the buffed version of authorize. So suppose you have user1, user2, org1, org2... you have access to org1, but you want to check if you have access to org2, you can configure autorepeater with options of match and replace Add a Replacement: \"Replace String\" - provide 1 org ID in Match and other org id in replace You can set highlight filter also in autorepeater. You can change content-length from application/json to application/xml in Request Header function of match and repleace.","title":"Stok IDOR Session"},{"location":"idor/#authorize-and-autorepeater-burp-plugins-to-find-idor","text":"","title":"Authorize and Autorepeater burp plugins to find IDOR."},{"location":"idor/#authorize","text":"For each request you do, it will send a equal request with something changed.. And this something is usually cookie of the session or any additional header. Example: User A will browse the webapp and we will use authorize to use cookies of user B automatically. Copy all the cookies if user 1, paste in authorize configuration In interception filter, add filters: Ignore spider request Scope items only URL Not Contains(regex): .Js|css|png|jpg|jpeg|gif|woff|BMP|ICO$ Start Authorize Browse Web using user1 and authorize will capture the request, and send the request again with modified cookies of user 2 Now you can click on each indiviual request to check modified response, if response is 403 forbidden that means user2 is not authorize to browse that webpage. 403 == Enforced == No Go So go through the webapp, browsing everything, changing profile and saving adn authorize will show if it is enforced or not. if it is not, try reading the response to check if it is vulnerable or not.","title":"Authorize"},{"location":"idor/#autorepeater","text":"Autorepeater is the buffed version of authorize. So suppose you have user1, user2, org1, org2... you have access to org1, but you want to check if you have access to org2, you can configure autorepeater with options of match and replace Add a Replacement: \"Replace String\" - provide 1 org ID in Match and other org id in replace You can set highlight filter also in autorepeater. You can change content-length from application/json to application/xml in Request Header function of match and repleace.","title":"Autorepeater"},{"location":"vim_tutorial/","text":"USing Assetfinder to get subdomains # assetfinder --subs-only uber.com domains Know which are running webservers, it checks which are having http and https server listening # cat domains | httpprobe | tee hosts /* Use \"tee -a\" to append */ Using meg # meg -d 1000 -c 10 / Above command will make an output direcotry called \"out\" and inside it, it will make individual directory for each subdomain. Each subdomain directory will contain a file for both http and https request. You also get a index.html file which will show the path with response code, so you can easily grep for 200 status code and get accepted urls. # grep -Hnri uber internal * | vim - you can save this result in a file \":w results\" Running commands in the vim :%!sort -u (% means current file, ! to run shell command) :%grep -v Content-Secu awk - text processing command :%!awk -F';' ;{print $3}' /* -F = Field Separator, '{print $X}' = print X field */ Ctrl+v/right click to put vim in visual mode and then you can use arrow keys to do vertical/horizontal selection and then hit 'X' to delete them Shift+a to go to the end of the line and delete a single item and then press dot(.) and vim will repeat the last thing(instruction) :/.$ (\\ - don't treat as an expression, $= End of the line) Search and replace :%s/\\ search_item>/\\ replace> :%s/// (// - it search for whatever you last searched for) xargs xargs takes multiple lines of input and runs a command on every line of it. :%!xargs -n1 -I{} sh -c 'echo {} | base64 -d' (n1 - give 1 input at a time, -I{} is a placeholder of input, sh -c - to pass the command to shell ) html-tool from tomnomnom # find . -type f | html-tool attribs src (this will give all of the src attributes from all of the files) # find . -type f | html-tool tags title | vim - ( give the title tag from all of the files) :tabnew | read !grep -HNR 'self' (grep for whatever under the cursor and then open the result in a new buffer in a new tab so it shows up at a few places then ctrl+w+g+shift+f and this will take me straight to the line where that title exists. gf gf urls | grep yahoo.com | vi - (finds every url) unfurl from tomnomnom (its a great tool to make wordlist using keys and values for fuzzing) :%!unfurl -u paths (gives the uniques paths from all the urls) :%!unfurl -u keys (to get query strings) param miner extension - finds hidden parameters, above will be a good wordlist","title":"Tomnomnom Vim Tutorial"},{"location":"vim_tutorial/#using-assetfinder-to-get-subdomains","text":"# assetfinder --subs-only uber.com domains","title":"USing Assetfinder to get subdomains"},{"location":"vim_tutorial/#know-which-are-running-webservers-it-checks-which-are-having-http-and-https-server-listening","text":"# cat domains | httpprobe | tee hosts /* Use \"tee -a\" to append */","title":"Know which are running webservers, it checks which are having http and https server listening"},{"location":"vim_tutorial/#using-meg","text":"# meg -d 1000 -c 10 / Above command will make an output direcotry called \"out\" and inside it, it will make individual directory for each subdomain. Each subdomain directory will contain a file for both http and https request. You also get a index.html file which will show the path with response code, so you can easily grep for 200 status code and get accepted urls. # grep -Hnri uber internal * | vim - you can save this result in a file \":w results\"","title":"Using meg"},{"location":"vim_tutorial/#running-commands-in-the-vim","text":":%!sort -u (% means current file, ! to run shell command) :%grep -v Content-Secu","title":"Running commands in the vim"},{"location":"vim_tutorial/#awk-text-processing-command","text":":%!awk -F';' ;{print $3}' /* -F = Field Separator, '{print $X}' = print X field */ Ctrl+v/right click to put vim in visual mode and then you can use arrow keys to do vertical/horizontal selection and then hit 'X' to delete them Shift+a to go to the end of the line and delete a single item and then press dot(.) and vim will repeat the last thing(instruction) :/.$ (\\ - don't treat as an expression, $= End of the line)","title":"awk - text processing command"},{"location":"vim_tutorial/#search-and-replace","text":":%s/\\ search_item>/\\ replace> :%s/// (// - it search for whatever you last searched for)","title":"Search and replace"},{"location":"vim_tutorial/#xargs","text":"xargs takes multiple lines of input and runs a command on every line of it. :%!xargs -n1 -I{} sh -c 'echo {} | base64 -d' (n1 - give 1 input at a time, -I{} is a placeholder of input, sh -c - to pass the command to shell )","title":"xargs"},{"location":"vim_tutorial/#html-tool-from-tomnomnom","text":"# find . -type f | html-tool attribs src (this will give all of the src attributes from all of the files) # find . -type f | html-tool tags title | vim - ( give the title tag from all of the files) :tabnew | read !grep -HNR 'self' (grep for whatever under the cursor and then open the result in a new buffer in a new tab so it shows up at a few places then ctrl+w+g+shift+f and this will take me straight to the line where that title exists.","title":"html-tool from tomnomnom"},{"location":"vim_tutorial/#gf","text":"gf urls | grep yahoo.com | vi - (finds every url)","title":"gf"},{"location":"vim_tutorial/#unfurl-from-tomnomnom-its-a-great-tool-to-make-wordlist-using-keys-and-values-for-fuzzing","text":":%!unfurl -u paths (gives the uniques paths from all the urls) :%!unfurl -u keys (to get query strings) param miner extension - finds hidden parameters, above will be a good wordlist","title":"unfurl from tomnomnom                     (its a great tool to make wordlist using keys and values for fuzzing)"},{"location":"bugbountynotes/challengesession1/","text":"Give some space to this XSS Filter Check the page source, read the JS code, understand it how it is working, how it is filtering the payload. try different payloads with tag on URI with fragment(#) On fileformat.info check for \"space\" in search box. And choose break space which in UTF8 is %C2%A0 Now try - onerror=%C2%A0a() Since above work, as checked in page source. we can try: #onerror=%C2%A0alert(socument.domain) Steal teh token! Use top.postmessage({uname:\"test\"},*) This strict URL filter should prevent XSS, right? solution: data:text/html,\\ iframe name=\"\\ svg onload=alert(document.domain)>\" src=https://www.bugbountytraining.com/challenges/challenge-6.php?url=javascript:name> explanation: When using the javascript pseudo scheme, if the returned value is a string, browsers will write it onto the page like document.write. This is why some bookmarklets have a void(0) at the end to prevent the results accidentally return a string. Another thing to know is the window.name property persists even after navigation, and we can control this value. So, we can assign window.name with a XSS payload in HTML, then use javascript:name so that it writes the payload onto the page.","title":"BugBountyNotes Challenge Session 1"},{"location":"bugbountynotes/challengesession1/#give-some-space-to-this-xss-filter","text":"Check the page source, read the JS code, understand it how it is working, how it is filtering the payload. try different payloads with tag on URI with fragment(#) On fileformat.info check for \"space\" in search box. And choose break space which in UTF8 is %C2%A0 Now try -","title":"Give some space to this XSS Filter"},{"location":"bugbountynotes/challengesession1/#onerrorc2a0a","text":"Since above work, as checked in page source. we can try: #onerror=%C2%A0alert(socument.domain)","title":"onerror=%C2%A0a()"},{"location":"bugbountynotes/challengesession1/#steal-teh-token","text":"Use top.postmessage({uname:\"test\"},*)","title":"Steal teh token!"},{"location":"bugbountynotes/challengesession1/#this-strict-url-filter-should-prevent-xss-right","text":"solution: data:text/html,\\ iframe name=\"\\ svg onload=alert(document.domain)>\" src=https://www.bugbountytraining.com/challenges/challenge-6.php?url=javascript:name> explanation: When using the javascript pseudo scheme, if the returned value is a string, browsers will write it onto the page like document.write. This is why some bookmarklets have a void(0) at the end to prevent the results accidentally return a string. Another thing to know is the window.name property persists even after navigation, and we can control this value. So, we can assign window.name with a XSS payload in HTML, then use javascript:name so that it writes the payload onto the page.","title":"This strict URL filter should prevent XSS, right?"},{"location":"bugbountynotes/fastfoodhackingsession1/","text":"Check for page source Check for any variable in script which is blacnk, most probably they are filled with URL parameters, so try those in query parameters. if some parameter is like \"trackingid\", and if it doesn't work, try other permutations of it like id, tid, tracking_id Use different user-agents like of iphone/android and see if there is any other parameter appear in the source code, use that in query parameter to check for XSS or injection. Check robots.txt page Got a login Page ?? Try admin,admin or other default passwords. if it works and you are logged in, try to access other pages like mentioned in robots.txt Try html tags in signup form: like : h2>test If they filter tags( ), then try h2, try only to check if this is filtered Check pagesource in settings page if they are hiding some button using comment, uncomment, and capture the request by sending the request. Try removing tokens, and check if they are vaidating tokens on server side. When you login, usually after successfull login website redirected you to some page like home page, check if you can change this redirect. Check for page source for any JS, see if there is any redirect url code in it. Try to capture login request and add redirect_url parameter. Add to the query statement of login.php page, like ?r_url=1 url=https:// Check in github for the website for any keys try query parameters like key, apiKey, apiSecret example: /?apiKey=123 apiSecret=2323 Just Try Things, if something you see in page source, thing about it, question yourself and try it.","title":"BugBountyNotes FastFoodHacking Session 1"},{"location":"bugbountynotes/fastfoodhackingsession1/#check-for-page-source","text":"Check for any variable in script which is blacnk, most probably they are filled with URL parameters, so try those in query parameters. if some parameter is like \"trackingid\", and if it doesn't work, try other permutations of it like id, tid, tracking_id Use different user-agents like of iphone/android and see if there is any other parameter appear in the source code, use that in query parameter to check for XSS or injection.","title":"Check for page source"},{"location":"bugbountynotes/fastfoodhackingsession1/#check-robotstxt-page","text":"","title":"Check robots.txt page"},{"location":"bugbountynotes/fastfoodhackingsession1/#got-a-login-page","text":"Try admin,admin or other default passwords. if it works and you are logged in, try to access other pages like mentioned in robots.txt Try html tags in signup form: like : h2>test If they filter tags( ), then try h2, try only to check if this is filtered","title":"Got a login Page ??"},{"location":"bugbountynotes/fastfoodhackingsession1/#check-pagesource-in-settings-page","text":"if they are hiding some button using comment, uncomment, and capture the request by sending the request. Try removing tokens, and check if they are vaidating tokens on server side.","title":"Check pagesource in settings page"},{"location":"bugbountynotes/fastfoodhackingsession1/#when-you-login-usually-after-successfull-login-website-redirected-you-to-some-page-like-home-page-check-if-you-can-change-this-redirect","text":"Check for page source for any JS, see if there is any redirect url code in it. Try to capture login request and add redirect_url parameter.","title":"When you login, usually after successfull login website redirected you to some page like home page, check if you can change this redirect."},{"location":"bugbountynotes/fastfoodhackingsession1/#add-to-the-query-statement-of-loginphp-page-like-r95url1urlhttps","text":"","title":"Add to the query statement of login.php page, like ?r_url=1&amp;url=https://"},{"location":"bugbountynotes/fastfoodhackingsession1/#check-in-github-for-the-website-for-any-keys","text":"try query parameters like key, apiKey, apiSecret example: /?apiKey=123 apiSecret=2323","title":"Check in github for the website for any keys"},{"location":"bugbountynotes/fastfoodhackingsession1/#just-try-things-if-something-you-see-in-page-source-thing-about-it-question-yourself-and-try-it","text":"","title":"Just Try Things, if something you see in page source, thing about it, question yourself and try it."},{"location":"bugbountynotes/fastfoodhackingsession2/","text":"Bug 1 Try this: /FFH/?tid=lol cmid=lol Above can be controlled as can be seen in page source. /FFH/?tid=lol\"-alert(1)-\"cmid=lol'-alert(0)' cmid worked since devloper was using only htmlentities. So if you have used \\ h2>xss it would not have worked since php htmlentities html encode the h2 tag. At tid, \\ script> is filtered, \\ \\script> is filtered, ' or \" everything is filterd. Try \\ h2>xss in tid /FFH/?tid=lol\\ h2>xss cmid=lol It worked so payload shoudl be at tid as : /FFH/?tid=\\ /script/x>\\ svg/onload=\"alert(1)\"> Bug 2 Log in and go to settings page. On the www.bugbountytraining.com/FFH/settings.php page: Send the name as xxx\" \\ h2>hi It gave invalid referrer error: Remove the referer and try the request again. Change the request method to GET Developer was simply checking for referer header. Bug 3 On the login form, you can see the page source, there is a Javascript that says if r_url=1 it will take url and put it in redirectURL param. /login.php?act-login token=yes url=http://google.com r_url=1 Below might bypass this filter as the devloper might be simply checking if url param is a url or not, if it is a url, block it. Developer use FILTER_VALIDATE_URL to check if its url or not. /login.php?act-login token=yes url=//google.com r_url=1 you can also double encode https://google.com to bypass the above filter. Bug 4 From robot.txt page, there is /book.php and /order.php pages available. Book something with test, test and capture the request, as you can see there is no CSRF protection token in this. We can generate the CSRF html payload and send the request using jsfiddle When we send the request from jsfiddle, we get error, check on burp what happened. As we can see the request, there is extra = coming in POST request, which is not valid JSON request. So we change our CSRF payload. = is coming because it is expcting value as we have put all payload in input name tag, we can put some stuff from the end of payload to value tag of payload. Developer is messed up here because they didn't validate Content-Type: which is text/plan in this case. They just checked the payload and they found it JSON so they passed it. Bug 5 Check the page source of book.php you cna see that there is a cancel button and there is href associated with it. /book.php?cancelUrl=javascript:alert(0) and we are able to change href using this and alert box open.","title":"BugBountyNotes FastFoodHacking Session 2"},{"location":"bugbountynotes/fastfoodhackingsession2/#bug-1","text":"Try this: /FFH/?tid=lol cmid=lol Above can be controlled as can be seen in page source. /FFH/?tid=lol\"-alert(1)-\"cmid=lol'-alert(0)' cmid worked since devloper was using only htmlentities. So if you have used \\ h2>xss it would not have worked since php htmlentities html encode the h2 tag. At tid, \\ script> is filtered, \\ \\script> is filtered, ' or \" everything is filterd. Try \\ h2>xss in tid /FFH/?tid=lol\\ h2>xss cmid=lol It worked so payload shoudl be at tid as : /FFH/?tid=\\ /script/x>\\ svg/onload=\"alert(1)\">","title":"Bug 1"},{"location":"bugbountynotes/fastfoodhackingsession2/#bug-2","text":"Log in and go to settings page. On the www.bugbountytraining.com/FFH/settings.php page: Send the name as xxx\" \\ h2>hi It gave invalid referrer error: Remove the referer and try the request again. Change the request method to GET Developer was simply checking for referer header.","title":"Bug 2"},{"location":"bugbountynotes/fastfoodhackingsession2/#bug-3","text":"On the login form, you can see the page source, there is a Javascript that says if r_url=1 it will take url and put it in redirectURL param. /login.php?act-login token=yes url=http://google.com r_url=1 Below might bypass this filter as the devloper might be simply checking if url param is a url or not, if it is a url, block it. Developer use FILTER_VALIDATE_URL to check if its url or not. /login.php?act-login token=yes url=//google.com r_url=1 you can also double encode https://google.com to bypass the above filter.","title":"Bug 3"},{"location":"bugbountynotes/fastfoodhackingsession2/#bug-4","text":"From robot.txt page, there is /book.php and /order.php pages available. Book something with test, test and capture the request, as you can see there is no CSRF protection token in this. We can generate the CSRF html payload and send the request using jsfiddle When we send the request from jsfiddle, we get error, check on burp what happened. As we can see the request, there is extra = coming in POST request, which is not valid JSON request. So we change our CSRF payload. = is coming because it is expcting value as we have put all payload in input name tag, we can put some stuff from the end of payload to value tag of payload. Developer is messed up here because they didn't validate Content-Type: which is text/plan in this case. They just checked the payload and they found it JSON so they passed it.","title":"Bug 4"},{"location":"bugbountynotes/fastfoodhackingsession2/#bug-5","text":"Check the page source of book.php you cna see that there is a cancel button and there is href associated with it. /book.php?cancelUrl=javascript:alert(0) and we are able to change href using this and alert box open.","title":"Bug 5"},{"location":"bugbountynotes/filterbypass/","text":"XSS Filter Are the basic \" ' \\ > allowed? what if you try in different combinations ? Is \"on[]=\" filtered or jut the common 'onerror', 'oncommon' filtered? try other such as onload, onhover -- put onxx= --- if this passes, that mean they are whitelisting few terms - remove = and try, sometimes dev people check for any statemement and they stop all statements. How do they handle unicode, double encoding, unusual encoding ? Note: *When you find a XSS filter bypass, it's usually common throughout because developers copy the same code through out application. Example: if - \\ img%20src=\"x\" onerror=\"alert(0)\"> encoded properly by developer try this: \\ img%20src='data:'onerror='alert(0)> try sscriptrscriptiscriptpscripttscript if the developer is blocking script tag the use of \\/\\/ will bypass the filter check for \"//\", and the use of \\n will bypass their check for .com, .co.uk etc.","title":"BugBountyNotes Filter Bypass Session"},{"location":"bugbountynotes/filterbypass/#xss-filter","text":"Are the basic \" ' \\ > allowed? what if you try in different combinations ? Is \"on[]=\" filtered or jut the common 'onerror', 'oncommon' filtered? try other such as onload, onhover -- put onxx= --- if this passes, that mean they are whitelisting few terms - remove = and try, sometimes dev people check for any statemement and they stop all statements. How do they handle unicode, double encoding, unusual encoding ? Note: *When you find a XSS filter bypass, it's usually common throughout because developers copy the same code through out application. Example: if - \\ img%20src=\"x\" onerror=\"alert(0)\"> encoded properly by developer try this: \\ img%20src='data:'onerror='alert(0)> try sscriptrscriptiscriptpscripttscript if the developer is blocking script tag the use of \\/\\/ will bypass the filter check for \"//\", and the use of \\n will bypass their check for .com, .co.uk etc.","title":"XSS Filter"},{"location":"bugbountynotes/firstlivementoring/","text":"-- Picking a target. It is tough to choose, but choose wisely. Poke a little bit on a target, find some low hanging fruit, understand the basic diffense the target is having. Get the feel for things on target. Do recon on the target, got subdomains now what?? -- it all depends now on the process that you want to follow or like: * You might only go for interesting names of subdomains and try to get some files. * You can find signup/registration adn login prompt subdomains and poke on these * Don't follow the complete process of others(hackers), make your own, take bit and pieces. Understand the target Learn how the developers are thinking, learn how the website is working. If the dev is missing something at one place, chances are that there will be some other places where they are missing the same. Understand what they are trying to protect, what they are trying to achieve. -- Never stop doing recon, websites are always bringing new code. -- XSS, open URL redirect, CSRF, clickjacking are low hanging fruits. -- Most apps are vulnerable to IDOR. Don't spray wild payloads everywhere, understand whats going on, make POC according to that. Try to understand what the website is actually about. There are som many bug class, so try to set your focus on what you what you want to find at the endpoint or in a website. If you find the key, google the key/token, check if there is some talk around it. Come up with your won checklists when testing ! Example: If you are abel to signup, you can interact, things to test for -stored xss, open redirect, can you upload photo, token leaks. Check what can you do? Look on burp, see interesting parameters. Example: This applies to XSS, open redirect - \\ h2> \\ h2 \\ %00h2> \\ %0dh2> \\ /h2/x> Try to understand the filter, try to understand if their filter is vulnerable. Put beeceptor kind of url in referer and check if site reaches to you. Use spider to find endpoints and JS links. Read, Read and read ! Disclose reports, tutorials, writeups, Test for bypasses ! Try Changing content-type Find the IP to bypass cloudfare. Found something in X-Forwarded-For or Cookie header which is exploting XSS, you can use cache poisoning, try it, it might work. How do you know if endpoint is vulnerable to SSRF? -- check for parameters, if something is URL, or something that is looking as url ,even if the endpoint like \"/endpoint\", potentially making a request somewhere to something, if you put your URL there, your server ip, or you can even put open url redirect so for example there's a end point where it will take another endpoint and send a request to it internally and give you the contents and if you try to change it to external website it doesn't work and it can only be an endpoint. Now if you found a open URL redirect what about if you force the server to visit that does it follow the redirect, which might follow it to somehting internal which might follow you the response. Example of Session Managament Issue I had a website where I could only insert an image but link to my website, I bypassed their protection to link it to my website and on the login they were pretty secure I can go to my URL and find some endpoints and one of the enpoint was where my image was so after logging in I get to that endpoint where my image was and then that image page link to my URL in the images and in the referer was leaked the user's token. Use X-Forwarded-For with 127.0.0.1 or local IP when you do signup/login/password reset and see what happens. There is no time for recon to stop, you can go as long as you can.","title":"BugBountyNotes Mentoring Session"},{"location":"ippsec/tmux/","text":"tmux runs as a process and not tied to a session, so if you make a tmux session it will not die when your ssh connection get closed, so you cna reconnect easily to earlier session without losing anything Create New tmux session # tmux new -s \\ name> example: # tmux new -s HTB Prefix key Ctrl + b === Prefix key To create a new window (prefixKey) c (prefixKey)0 or (prefixKey)1 to switch to different sessions tmux config #vi ~/.tmux.conf #Remap prefix to screens set -g prefix C-a bind C-a send-prefix unbind C-b #Quality of life stuff set -g history-limit 10000 set -g aloow-rename off ## Join Windows bind-key j commnad-prompt -p \"join pane from:\" \"join-pane -s '%%'\" bind-key s commnad-prompt -p \"send pane to:\" \"join-pane -t '%%'\" # Search Mode VI (default is emac) set-window-option -g mode-keys vi run-shell /opt/tmux-logging/logging.tmux To Attach to a Session #tmux ls # tmux attach -t \\ session_name> To Detach from a Session (prefixKey)d Rename a session (prefixKey), Send window to a pane (prefixKey)s Move to Copy Mode (prefixKey)[ now you can use page up and page down to move around Hit \\ space> to enter into copy mode, select text, hit \\ enter> to copy into the vim buffer Now open vim (prefixKey)] -- and it paste TO Save logs (prefixKey) + Alt+Shift+P Veritcal split (prefixKey)% Horizontal Split (prefixKey)\" To move around in windows (prefixKey)\\ arrow_key> Zoom in to any windows (prefixKey)z --- Do this again to zoom out TO resize a window (prefixKey) + \\ Arrows> --- hold ctrl in this case (prefixKey){ -- to move the window to other layout (prefixKey)} -- move the window to other layout (prefixKey)\\ space> -- to change the layout","title":"ippsec tmux Session"},{"location":"ippsec/tmux/#tmux-runs-as-a-process-and-not-tied-to-a-session-so-if-you-make-a-tmux-session-it-will-not-die-when-your-ssh-connection-get-closed-so-you-cna-reconnect-easily-to-earlier-session-without-losing-anything","text":"","title":"tmux runs as a process and not tied to a session, so if you make a tmux session it will not die when your ssh connection get closed, so you cna reconnect easily to earlier session without losing anything"},{"location":"ippsec/tmux/#create-new-tmux-session","text":"# tmux new -s \\ name> example: # tmux new -s HTB","title":"Create New tmux session"},{"location":"ippsec/tmux/#prefix-key","text":"Ctrl + b === Prefix key","title":"Prefix key"},{"location":"ippsec/tmux/#to-create-a-new-window","text":"(prefixKey) c (prefixKey)0 or (prefixKey)1 to switch to different sessions","title":"To create a new window"},{"location":"ippsec/tmux/#tmux-config","text":"#vi ~/.tmux.conf #Remap prefix to screens set -g prefix C-a bind C-a send-prefix unbind C-b #Quality of life stuff set -g history-limit 10000 set -g aloow-rename off ## Join Windows bind-key j commnad-prompt -p \"join pane from:\" \"join-pane -s '%%'\" bind-key s commnad-prompt -p \"send pane to:\" \"join-pane -t '%%'\" # Search Mode VI (default is emac) set-window-option -g mode-keys vi run-shell /opt/tmux-logging/logging.tmux","title":"tmux config"},{"location":"ippsec/tmux/#to-attach-to-a-session","text":"#tmux ls # tmux attach -t \\ session_name>","title":"To Attach to a Session"},{"location":"ippsec/tmux/#to-detach-from-a-session","text":"(prefixKey)d","title":"To Detach from a Session"},{"location":"ippsec/tmux/#rename-a-session","text":"(prefixKey),","title":"Rename a session"},{"location":"ippsec/tmux/#send-window-to-a-pane","text":"(prefixKey)s","title":"Send window to a pane"},{"location":"ippsec/tmux/#move-to-copy-mode","text":"(prefixKey)[ now you can use page up and page down to move around Hit \\ space> to enter into copy mode, select text, hit \\ enter> to copy into the vim buffer Now open vim (prefixKey)] -- and it paste","title":"Move to Copy Mode"},{"location":"ippsec/tmux/#to-save-logs","text":"(prefixKey) + Alt+Shift+P","title":"TO Save logs"},{"location":"ippsec/tmux/#veritcal-split","text":"(prefixKey)%","title":"Veritcal split"},{"location":"ippsec/tmux/#horizontal-split","text":"(prefixKey)\"","title":"Horizontal Split"},{"location":"ippsec/tmux/#to-move-around-in-windows","text":"(prefixKey)\\ arrow_key>","title":"To move around in windows"},{"location":"ippsec/tmux/#zoom-in-to-any-windows","text":"(prefixKey)z --- Do this again to zoom out","title":"Zoom in to any windows"},{"location":"ippsec/tmux/#to-resize-a-window","text":"(prefixKey) + \\ Arrows> --- hold ctrl in this case (prefixKey){ -- to move the window to other layout (prefixKey)} -- move the window to other layout (prefixKey)\\ space> -- to change the layout","title":"TO resize a window"},{"location":"nahamsec/recon_session_1/","text":"Recon Session 1 on Yahoo Check for In-Scope and Out-of -Scope items on Hackerone.com Search on crt.sh --- %.yahoo.com -- you can put % wildcard anywhere in search like %api.yahoo.com -- %internal%.yahoo.com Search on certspotter --- yahoo.com Check the github code in recon_profile repo of nahamsec for cerspotter bash 1 line command, you can set it in your .bash_profile and then run it just by calling function name # certspotter yahoo.com -- output of this will not give all the subdomains of yahoo Check for first level subdomain example: ops.yahoo.com -- juicy + potentiall internal ne1.yahoo.com -- corporate/internal domain xobni.yahoo.com -- next potential internal Do These First corp.yahoo.com bf1.yahoo.com urs.yahoo.com adx.yahoo.com Do later gq1.yahoo.com ir2/yahoo.com sg3.yahoo.com tp2.yahoo.com udb.yahoo.com -- api enpoints payment.yahoo.com --- associated with internalapi * these are all interesting as these belong to corporate domains. Check for how many subdomains are alive/online ? certspotter corp.yahoo.com | httprobe httprobe os from tomnonnon github Make directory for target and all subdomains and all alive domains: # mkdir yahoo.com # cd yahoo.com # touch all.txt # touch alive.txt We are picking up api endpoints because these give really cool interesting stuff, internals and corporate because it is internal to yahoo and we want to know if we have access to it from outside. # certspotter yahoo.com | grep dev | httprobe # certspotter assistant.yahoo.com | grep dev | httprobe open a page from here on browser to check the response Note: Error 404 does not mean that we don't have anything there You can have a simple for loop to run other commands example: for i in certspotter assistant.yahoo.com | httprobe ; do curl $i/phpinfo.php; done Template for i in certspotter assistant.yahoo.com | httprobe ; do ; done # crtsh ops.yahoo.com all.txt # crtsh corp.yahoo.com all.txt Process is to able to find interesting things and how infrastructure of a website is built. Check for xobni in cr.sh like %xobni%.yahoo.com # crtsh xobni.yahoo.com all.txt # nano ~/.bash_profile getcount(){ cat all.txt | wc -l } # source ~/.bash_profile # cat all.txt | httprobe Put some more wildcard in crt.sh website ---- %25.%25.%25.%25.%25.yahoo.com Got some interesting subdomains: splunk.yahoo.com test-paranoids.yahoo.com manhattan.tp2.yahoo.com # crtsh test-paranoids.yahoo.com | httprobe ---- gave no result, so offline for us and of no use # crtsh bf1.yahoo.com | wc -l -- 509 # crtsh bf1.yahoo.com Check on crt.sh website for %.bf1.yahoo.com corp.bf1.yahoo.com mail.bf1.yahoo.com infra.bf1.yahoo.com vip.bf1.yahoo.com openstack.bf1.yahoo.com cs.bf1.yahoo.com flurry.bf1.yahoo.com prod.bf1.yahoo.com azurite.bf1.yahoo.com omega.bf1.yahoo.com ideas %uat% run a curl on adx.bf1.yahoo.com/pguix Google Dorks site:bf1.yahoo.com -flickr -omega -adx \"bf1.yahoo.com\" github.com # vi bf1 -- dump all bf1 subdomains here for i in `cmd`; do crtsh $i ; done Run crtsh on all the above bf1 domain Get some more interesting domains. iextract.bf1.yahoo.com diy.bf1.yahoo.com manhatan.bf1.yahoo.com netops.ne1.yahoo.com hk.%.yahoo.com advertising.yahoo.com creative.yahoo.com Check for Some interesting subdomains containing these keywords admin jenkins stage test dev devops staff db qa internal Now find all the alive domains # cat all.txt | httprobe alive.txt Check http probe on all ports except port 80 and 443 # cat all.txt | httprove -s -p https:8443 Take Screenshots now Install phantomJS # python webscreenshot.py \\ input_dir> -o \\ out_dir> -w 20 -m -a \"X-FORWARDED-FOR:127.0.0.1\" for I in $(ls); do echo $I index.html echo img src=$I br index.html; done for I in $(cat alive.txt); do python3 dirsearch ... -u https://$I done Learning: Learn the process of recon, every one has or can use their own tool, don't adopt tools, think about the process that you can take during your recon.","title":"NahamSec Recon Session 1 Notes"},{"location":"nahamsec/recon_session_1/#recon-session-1-on-yahoo","text":"","title":"Recon Session 1 on Yahoo"},{"location":"nahamsec/recon_session_1/#check-for-in-scope-and-out-of-scope-items-on-hackeronecom","text":"","title":"Check for In-Scope and Out-of -Scope items on Hackerone.com"},{"location":"nahamsec/recon_session_1/#search-on-crtsh-yahoocom","text":"-- you can put % wildcard anywhere in search like %api.yahoo.com -- %internal%.yahoo.com","title":"Search on crt.sh    ---&gt; %.yahoo.com"},{"location":"nahamsec/recon_session_1/#search-on-certspotter-yahoocom","text":"Check the github code in recon_profile repo of nahamsec for cerspotter bash 1 line command, you can set it in your .bash_profile and then run it just by calling function name # certspotter yahoo.com -- output of this will not give all the subdomains of yahoo","title":"Search on certspotter   ---&gt; yahoo.com"},{"location":"nahamsec/recon_session_1/#check-for-first-level-subdomain-example","text":"ops.yahoo.com -- juicy + potentiall internal ne1.yahoo.com -- corporate/internal domain xobni.yahoo.com -- next potential internal Do These First corp.yahoo.com bf1.yahoo.com urs.yahoo.com adx.yahoo.com Do later gq1.yahoo.com ir2/yahoo.com sg3.yahoo.com tp2.yahoo.com udb.yahoo.com -- api enpoints payment.yahoo.com --- associated with internalapi * these are all interesting as these belong to corporate domains.","title":"Check for first level subdomain example:"},{"location":"nahamsec/recon_session_1/#check-for-how-many-subdomains-are-aliveonline","text":"certspotter corp.yahoo.com | httprobe httprobe os from tomnonnon github","title":"Check for how many subdomains are alive/online ?"},{"location":"nahamsec/recon_session_1/#make-directory-for-target-and-all-subdomains-and-all-alive-domains","text":"# mkdir yahoo.com # cd yahoo.com # touch all.txt # touch alive.txt We are picking up api endpoints because these give really cool interesting stuff, internals and corporate because it is internal to yahoo and we want to know if we have access to it from outside. # certspotter yahoo.com | grep dev | httprobe # certspotter assistant.yahoo.com | grep dev | httprobe open a page from here on browser to check the response Note: Error 404 does not mean that we don't have anything there","title":"Make directory for target and all subdomains and all alive domains:"},{"location":"nahamsec/recon_session_1/#you-can-have-a-simple-for-loop-to-run-other-commands","text":"example: for i in certspotter assistant.yahoo.com | httprobe ; do curl $i/phpinfo.php; done Template for i in certspotter assistant.yahoo.com | httprobe ; do ; done # crtsh ops.yahoo.com all.txt # crtsh corp.yahoo.com all.txt Process is to able to find interesting things and how infrastructure of a website is built. Check for xobni in cr.sh like %xobni%.yahoo.com # crtsh xobni.yahoo.com all.txt # nano ~/.bash_profile getcount(){ cat all.txt | wc -l } # source ~/.bash_profile # cat all.txt | httprobe Put some more wildcard in crt.sh website ---- %25.%25.%25.%25.%25.yahoo.com Got some interesting subdomains: splunk.yahoo.com test-paranoids.yahoo.com manhattan.tp2.yahoo.com # crtsh test-paranoids.yahoo.com | httprobe ---- gave no result, so offline for us and of no use # crtsh bf1.yahoo.com | wc -l -- 509 # crtsh bf1.yahoo.com","title":"You can have a simple for loop to run other commands"},{"location":"nahamsec/recon_session_1/#check-on-crtsh-website-for-bf1yahoocom","text":"corp.bf1.yahoo.com mail.bf1.yahoo.com infra.bf1.yahoo.com vip.bf1.yahoo.com openstack.bf1.yahoo.com cs.bf1.yahoo.com flurry.bf1.yahoo.com prod.bf1.yahoo.com azurite.bf1.yahoo.com omega.bf1.yahoo.com ideas %uat% run a curl on adx.bf1.yahoo.com/pguix","title":"Check on crt.sh website for %.bf1.yahoo.com"},{"location":"nahamsec/recon_session_1/#google-dorks","text":"site:bf1.yahoo.com -flickr -omega -adx \"bf1.yahoo.com\" github.com # vi bf1 -- dump all bf1 subdomains here for i in `cmd`; do crtsh $i ; done Run crtsh on all the above bf1 domain Get some more interesting domains. iextract.bf1.yahoo.com diy.bf1.yahoo.com manhatan.bf1.yahoo.com netops.ne1.yahoo.com hk.%.yahoo.com advertising.yahoo.com creative.yahoo.com","title":"Google Dorks"},{"location":"nahamsec/recon_session_1/#check-for-some-interesting-subdomains-containing-these-keywords","text":"admin jenkins stage test dev devops staff db qa internal","title":"Check for Some interesting subdomains containing these keywords"},{"location":"nahamsec/recon_session_1/#now-find-all-the-alive-domains","text":"# cat all.txt | httprobe alive.txt","title":"Now find all the alive domains"},{"location":"nahamsec/recon_session_1/#check-http-probe-on-all-ports-except-port-80-and-443","text":"# cat all.txt | httprove -s -p https:8443","title":"Check http probe on all ports except port 80 and 443"},{"location":"nahamsec/recon_session_1/#take-screenshots-now","text":"","title":"Take Screenshots now"},{"location":"nahamsec/recon_session_1/#install-phantomjs","text":"# python webscreenshot.py \\ input_dir> -o \\ out_dir> -w 20 -m -a \"X-FORWARDED-FOR:127.0.0.1\" for I in $(ls); do echo $I index.html echo img src=$I br index.html; done for I in $(cat alive.txt); do python3 dirsearch ... -u https://$I done","title":"Install phantomJS"},{"location":"nahamsec/recon_session_1/#learning","text":"","title":"Learning:"},{"location":"nahamsec/recon_session_1/#learn-the-process-of-recon-every-one-has-or-can-use-their-own-tool-dont-adopt-tools-think-about-the-process-that-you-can-take-during-your-recon","text":"","title":"Learn the process of recon, every one has or can use their own tool, don't adopt tools, think about the process that you can take during your recon."},{"location":"nahamsec/recon_session_2/","text":"This Week's Topics: crt.sh certspotter.com Shodan Censys.io All Subdomains and Data: https://github.com/nahamsec/SundayStr... Github: https://github.com/nahamsec Tools: JSParser: https://github.com/nahamsec/JSParser LazyRecon: https://github.com/nahamsec/LazyRecon Bash Aliases: https://github.com/nahamsec/recon_pro... Webscreenshot: https://github.com/maaaaz/webscreenshot httprobe: https://github.com/tomnomnom/httprobe aquatone: https://github.com/michenriksen/aquatone --- anew from tomnomnom, append lines from stdin to a file, but only if they don't already appear in the file # cat things.txt # cat newthings.txt # cat newthings.txt | anew things.txt Check meg tool also from tomnomnom Get jq from here - https://stedolan.github.io/jq/ -- jq is like sed for JSON data - you can use it to slice and filter and map and transform structured data `` for i in cat target.txt` do curl -s 'https://crt.sh/\\?q\\=\\%25.$i\\ output\\=json' | jq -r '.[].name_value' | sed 's/*.//g' | sort -u |tee -a all.txt done Censys Search for 443.https.tls.chain.parsed.names:yahoo.com ToDo: ysm.yahoo.com bfv.yahoo.com cp.yahoo.com adspecs.yahoo.com yahoo-inc.com admanageplus.com tw.yahoo.com vpg.yahoo.com uad2.yahoo.com ah.yahoo.com abuse.yahoo.com sk1.yahoo.com sync.yahoo.com hk.%25.yahoo.com sports.yahoo.com sg3.yahoo.com sg3.yahoo.com yql.yahoo.com esports.yahoo.com api.fantasysports.yahoo.com mobile.yahoo.com data.yahoo.com sandbox.yahoo.com %answers%.yahoo.com geo.yahoo.com groups.yahoo.com gemini.yahoo.com admanager.yahoo.com Country based Searches %25.ect.yahoo.com %25.eds.yahoo.com %25.tw1.yahoo.com %25.bid.yahoo.com hk.%25.biling.yahoo.com hk.%25admin.yahoo.com hk.%25.deals.yahoo.com hk.%25.token.yahoo.com \\%25.hk.\\%25.yahoo.com hk.%25api.token.yahoo.com hk.%25.auctions.yahoo.com yw.%25.yahoo.com tw.%25.mall.yahoo.com hk.%25.store.auctions.yahoo.com tw.back ideas / future domains diy.bf1.yahoo.com yahooapis.com verizonmedia/oauth.com ????? txmblr.com t.umblr.com srvcs.tumblr.com makers.com builtbygirls.com -- Check in Certificated of Censys for yahoo.com Gogle Search site:yahoo.com ext:php -hk -tw -maktoob Aquatone and gowitness for screenshot of websites # cat alive.txt | aquatone -chromepath \\ chromium_path> -out \\ out_dir> -threads 50 Search on shodan for ipv4 addresses of yahoo Aproach Find Subdomains - port scan - screenshot - ditbrute force - Hack all the things","title":"NahamSec Recon Session 2 Notes"},{"location":"nahamsec/recon_session_2/#this-weeks-topics","text":"crt.sh certspotter.com Shodan Censys.io All Subdomains and Data: https://github.com/nahamsec/SundayStr... Github: https://github.com/nahamsec Tools: JSParser: https://github.com/nahamsec/JSParser LazyRecon: https://github.com/nahamsec/LazyRecon Bash Aliases: https://github.com/nahamsec/recon_pro... Webscreenshot: https://github.com/maaaaz/webscreenshot httprobe: https://github.com/tomnomnom/httprobe aquatone: https://github.com/michenriksen/aquatone --- anew from tomnomnom, append lines from stdin to a file, but only if they don't already appear in the file # cat things.txt # cat newthings.txt # cat newthings.txt | anew things.txt","title":"This Week's Topics:"},{"location":"nahamsec/recon_session_2/#check-meg-tool-also-from-tomnomnom","text":"","title":"Check meg tool also from tomnomnom"},{"location":"nahamsec/recon_session_2/#get-jq-from-here-httpsstedolangithubiojq-jq-is-like-sed-for-json-data-you-can-use-it-to-slice-and-filter-and-map-and-transform-structured-data","text":"`` for i in cat target.txt` do curl -s 'https://crt.sh/\\?q\\=\\%25.$i\\ output\\=json' | jq -r '.[].name_value' | sed 's/*.//g' | sort -u |tee -a all.txt done","title":"Get jq from here - https://stedolan.github.io/jq/      --&gt; jq is like sed for JSON data - you can use it to slice and filter and map and transform structured data"},{"location":"nahamsec/recon_session_2/#censys","text":"Search for 443.https.tls.chain.parsed.names:yahoo.com","title":"Censys"},{"location":"nahamsec/recon_session_2/#todo","text":"ysm.yahoo.com bfv.yahoo.com cp.yahoo.com adspecs.yahoo.com yahoo-inc.com admanageplus.com tw.yahoo.com vpg.yahoo.com uad2.yahoo.com ah.yahoo.com abuse.yahoo.com sk1.yahoo.com sync.yahoo.com hk.%25.yahoo.com sports.yahoo.com sg3.yahoo.com sg3.yahoo.com yql.yahoo.com esports.yahoo.com api.fantasysports.yahoo.com mobile.yahoo.com data.yahoo.com sandbox.yahoo.com %answers%.yahoo.com geo.yahoo.com groups.yahoo.com gemini.yahoo.com admanager.yahoo.com","title":"ToDo:"},{"location":"nahamsec/recon_session_2/#country-based-searches","text":"%25.ect.yahoo.com %25.eds.yahoo.com %25.tw1.yahoo.com %25.bid.yahoo.com hk.%25.biling.yahoo.com hk.%25admin.yahoo.com hk.%25.deals.yahoo.com hk.%25.token.yahoo.com \\%25.hk.\\%25.yahoo.com hk.%25api.token.yahoo.com hk.%25.auctions.yahoo.com yw.%25.yahoo.com tw.%25.mall.yahoo.com hk.%25.store.auctions.yahoo.com tw.back","title":"Country based Searches"},{"location":"nahamsec/recon_session_2/#ideas-future-domains","text":"diy.bf1.yahoo.com yahooapis.com verizonmedia/oauth.com ????? txmblr.com t.umblr.com srvcs.tumblr.com makers.com builtbygirls.com -- Check in Certificated of Censys for yahoo.com","title":"ideas / future domains"},{"location":"nahamsec/recon_session_2/#gogle-search","text":"site:yahoo.com ext:php -hk -tw -maktoob","title":"Gogle Search"},{"location":"nahamsec/recon_session_2/#aquatone-and-gowitness-for-screenshot-of-websites","text":"# cat alive.txt | aquatone -chromepath \\ chromium_path> -out \\ out_dir> -threads 50","title":"Aquatone and gowitness for screenshot of websites"},{"location":"nahamsec/recon_session_2/#search-on-shodan-for-ipv4-addresses-of-yahoo","text":"","title":"Search on shodan for ipv4 addresses of yahoo"},{"location":"nahamsec/recon_session_2/#aproach","text":"Find Subdomains - port scan - screenshot - ditbrute force - Hack all the things","title":"Aproach"},{"location":"tomnomnom/session1/","text":"vi tom_tools put some tools inside the above file cat tom_tools| xargs -n1 -I{} go get github.com/tomnomnom/{} echo$? -- for checking the last command return value Using Assetfinder # assetfinder --subs-only sports.yahoo.com | tee -a domain HttProbe #cat domain | httprobe -c 50 | tee -a hosts meg # meg -v / / in same direcotory as hosts file / cd out/ grep -Hnri aws_ grep -Hnri secret find . type f | html-tool tags title find . type f | html-tool attribs src vimprev script !/bin/bash VIMENV=prev vim $@ vimrc #vi ~/.vimrc if $VIMENV == 'talk' set background=light let g:solarized_termcolors=256 colo solarized noremap \\ Space> :n\\ CR> noremap \\ Backspace> :N\\ CR> else \" Trans background hi Normal ctermbg=none hi NonText ctermg=none endif if $VIMENV == 'prev' noremap \\ Space> :n\\ CR> noremap \\ Backspace> :N\\ CR> set noswapfile endif Using above vimprev script vimprev $(find . -type f) Now I can use \"space\" to move to next file and backspace to move to prev one. We often see many pages return \"200 OK\" but really they are not. # grep -Hnri '200 Ok' | grep -v ^index # grep -lru '200 OK' | grep -v ^index | xargs -n1 ls -la # grep -lru '200 OK' | grep -v ^index | xargs -n1 ls -la | sort -k5,5 Now above output will sort according to length so you can check for interesting length which # find . -type f -exec cat {} \\; | sort --version-sort -u | wc # find . -type f -exec cat {} \\; | sort --version-sort -u | vim - Using tok Break stream of inputs into words. # find . -type f -exec cat {} \\; | tok | vim - :%!sort -u --version-sort to sort them unique - then you can grep any interesting word in the host folder to check easily where it belongs and check further from here. Deal with a very large file Sort the file first in vim %!sort -u --version-sort cat urls | urlinteresting | vim - To check for subdomain takeover # cat domains | while read domain; do host -t CNAME $domain; done | grep -i azure","title":"Tomnomnom Q&A Session"},{"location":"tomnomnom/session1/#using-assetfinder","text":"# assetfinder --subs-only sports.yahoo.com | tee -a domain","title":"Using Assetfinder"},{"location":"tomnomnom/session1/#httprobe","text":"#cat domain | httprobe -c 50 | tee -a hosts","title":"HttProbe"},{"location":"tomnomnom/session1/#meg","text":"# meg -v / / in same direcotory as hosts file / cd out/ grep -Hnri aws_ grep -Hnri secret find . type f | html-tool tags title find . type f | html-tool attribs src","title":"meg"},{"location":"tomnomnom/session1/#vimprev-script","text":"","title":"vimprev script"},{"location":"tomnomnom/session1/#binbash","text":"VIMENV=prev vim $@","title":"!/bin/bash"},{"location":"tomnomnom/session1/#vimrc","text":"#vi ~/.vimrc if $VIMENV == 'talk' set background=light let g:solarized_termcolors=256 colo solarized noremap \\ Space> :n\\ CR> noremap \\ Backspace> :N\\ CR> else \" Trans background hi Normal ctermbg=none hi NonText ctermg=none endif if $VIMENV == 'prev' noremap \\ Space> :n\\ CR> noremap \\ Backspace> :N\\ CR> set noswapfile endif","title":"vimrc"},{"location":"tomnomnom/session1/#using-above-vimprev-script","text":"vimprev $(find . -type f) Now I can use \"space\" to move to next file and backspace to move to prev one. We often see many pages return \"200 OK\" but really they are not. # grep -Hnri '200 Ok' | grep -v ^index # grep -lru '200 OK' | grep -v ^index | xargs -n1 ls -la # grep -lru '200 OK' | grep -v ^index | xargs -n1 ls -la | sort -k5,5 Now above output will sort according to length so you can check for interesting length which # find . -type f -exec cat {} \\; | sort --version-sort -u | wc # find . -type f -exec cat {} \\; | sort --version-sort -u | vim -","title":"Using above vimprev script"},{"location":"tomnomnom/session1/#using-tok","text":"Break stream of inputs into words. # find . -type f -exec cat {} \\; | tok | vim - :%!sort -u --version-sort to sort them unique - then you can grep any interesting word in the host folder to check easily where it belongs and check further from here.","title":"Using tok"},{"location":"tomnomnom/session1/#deal-with-a-very-large-file","text":"Sort the file first in vim %!sort -u --version-sort cat urls | urlinteresting | vim -","title":"Deal with a very large file"},{"location":"tomnomnom/session1/#to-check-for-subdomain-takeover","text":"# cat domains | while read domain; do host -t CNAME $domain; done | grep -i azure","title":"To check for subdomain takeover"}]}